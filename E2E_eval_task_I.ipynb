{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E2E_eval_task_I.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMBSiib2FpgKVr8bk1eYVJq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AritraStark/E2E_GSOC_2022/blob/main/E2E_eval_task_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Task 1. Electron/photon classification**\n",
        "\n",
        "Datasets:\n",
        "\n",
        "https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc (photons)\n",
        "\n",
        "https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA (electrons)\n",
        "\n",
        "Description: 32x32 matrices (two channels - hit energy and time) for two classes of particles electrons and photons impinging on a calorimeter\n",
        "Please use a deep learning method of your choice to achieve the highest possible\n",
        "classification on this dataset (we ask that you do it both in Keras/Tensorflow and in PyTorch). Please provide a Jupyter notebook that shows your solution. The model yousubmit should have a ROC AUC score of at least 0.80."
      ],
      "metadata": {
        "id": "vsTucLeyQCsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the datasets from the links provided: "
      ],
      "metadata": {
        "id": "VBwlRChLQCga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc/download -O photons.hdf5\n",
        "!wget https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA/download -O electrons.hdf5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaIFeaOAc4bN",
        "outputId": "9ef3630c-ca8e-49cf-89e6-3a0903368d61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-21 17:22:32--  https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc/download\n",
            "Resolving cernbox.cern.ch (cernbox.cern.ch)... 128.142.53.28, 128.142.53.35, 137.138.120.151, ...\n",
            "Connecting to cernbox.cern.ch (cernbox.cern.ch)|128.142.53.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119703858 (114M) [application/octet-stream]\n",
            "Saving to: ‘photons.hdf5’\n",
            "\n",
            "photons.hdf5        100%[===================>] 114.16M   127MB/s    in 0.9s    \n",
            "\n",
            "Last-modified header invalid -- time-stamp ignored.\n",
            "2022-03-21 17:22:34 (127 MB/s) - ‘photons.hdf5’ saved [119703858/119703858]\n",
            "\n",
            "--2022-03-21 17:22:35--  https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA/download\n",
            "Resolving cernbox.cern.ch (cernbox.cern.ch)... 128.142.53.28, 128.142.53.35, 137.138.120.151, ...\n",
            "Connecting to cernbox.cern.ch (cernbox.cern.ch)|128.142.53.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128927319 (123M) [application/octet-stream]\n",
            "Saving to: ‘electrons.hdf5’\n",
            "\n",
            "electrons.hdf5      100%[===================>] 122.95M   118MB/s    in 1.0s    \n",
            "\n",
            "Last-modified header invalid -- time-stamp ignored.\n",
            "2022-03-21 17:22:37 (118 MB/s) - ‘electrons.hdf5’ saved [128927319/128927319]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the imports:"
      ],
      "metadata": {
        "id": "KKyu1QBLc9Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import h5py\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbTnS9MPdCCm",
        "outputId": "2fef8416-651e-41f0-9232-7d211b5d8625"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the data from the downloaded HDF5 files and combine the loaded datasets:"
      ],
      "metadata": {
        "id": "9X8Ns4T3ebZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_electron = np.array(h5py.File(\"electrons.hdf5\",'r').get(name=\"X\")[()])\n",
        "y_electron = np.array(h5py.File(\"electrons.hdf5\",'r').get(name=\"y\")[()])\n",
        "X_photon = np.array(h5py.File(\"photons.hdf5\",'r').get(name=\"X\")[()])\n",
        "y_photon = np.array(h5py.File(\"photons.hdf5\",'r').get(name=\"y\")[()])\n",
        "\n",
        "X_particles = np.concatenate((X_electron,X_photon),axis=0)\n",
        "y_particles = np.concatenate((y_electron,y_photon),axis=0)\n",
        "print(X_particles.shape,y_particles.shape)\n",
        "\n",
        "del X_electron\n",
        "del X_photon\n",
        "del y_electron\n",
        "del y_photon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNhJPOwKeldg",
        "outputId": "47f61794-9311-417b-8521-99f9ca381111"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(498000, 32, 32, 2) (498000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening the data: "
      ],
      "metadata": {
        "id": "_im73IfrZfuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stream_data = X_particles[:,:,:,0].reshape(-1,32*32)\n",
        "stream_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG_hh9yRZiEC",
        "outputId": "008e3f10-5452-4055-a665-1fb9e2e98b59"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(498000, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Randomizing data:"
      ],
      "metadata": {
        "id": "33iGX-LnY-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(48)\n",
        "rng_state = np.random.get_state()\n",
        "np.random.shuffle(stream_data)\n",
        "np.random.set_state(rng_state)\n",
        "np.random.shuffle(y_particles)"
      ],
      "metadata": {
        "id": "rwukmb7yZBgy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the data into training and testing sets ( I have split it in 80-20 as per instructions): "
      ],
      "metadata": {
        "id": "B_96cSfhb3G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split( stream_data, y_particles, random_state=48, test_size=0.2 )\n"
      ],
      "metadata": {
        "id": "hyDuWD1Sb6tY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model:"
      ],
      "metadata": {
        "id": "AEYtfPYbco75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(512, activation = 'relu', input_shape =X_train.shape[1:]),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(256, activation = 'relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(128, activation = 'relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(64, activation = 'relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "GjZA7FP8cyMg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining callbacks:"
      ],
      "metadata": {
        "id": "fEdVlpnygsgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"classifier_weights2-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint1 = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1]"
      ],
      "metadata": {
        "id": "9VM4NKsZgu8Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model and fitting it with testing data:"
      ],
      "metadata": {
        "id": "9SFMBIyAgZ2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(X_train,  y_train, \n",
        "                    validation_split=0.2, \n",
        "                    epochs=200, \n",
        "                    batch_size=2000,\n",
        "                    callbacks=callbacks_list)"
      ],
      "metadata": {
        "id": "GBlYFXwGgf3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11ce789-ff9f-40c7-eb04-555ed027d9a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.6487 - accuracy: 0.6227\n",
            "Epoch 1: val_accuracy improved from -inf to 0.69004, saving model to classifier_weights2-improvement-01-0.69.hdf5\n",
            "160/160 [==============================] - 4s 19ms/step - loss: 0.6481 - accuracy: 0.6235 - val_loss: 0.5976 - val_accuracy: 0.6900\n",
            "Epoch 2/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5983 - accuracy: 0.6909\n",
            "Epoch 2: val_accuracy improved from 0.69004 to 0.70747, saving model to classifier_weights2-improvement-02-0.71.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5983 - accuracy: 0.6909 - val_loss: 0.5777 - val_accuracy: 0.7075\n",
            "Epoch 3/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.7051\n",
            "Epoch 3: val_accuracy did not improve from 0.70747\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5833 - accuracy: 0.7051 - val_loss: 0.5775 - val_accuracy: 0.7074\n",
            "Epoch 4/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.7102\n",
            "Epoch 4: val_accuracy improved from 0.70747 to 0.71678, saving model to classifier_weights2-improvement-04-0.72.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5773 - accuracy: 0.7102 - val_loss: 0.5665 - val_accuracy: 0.7168\n",
            "Epoch 5/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5755 - accuracy: 0.7122\n",
            "Epoch 5: val_accuracy did not improve from 0.71678\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5755 - accuracy: 0.7121 - val_loss: 0.5669 - val_accuracy: 0.7147\n",
            "Epoch 6/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5715 - accuracy: 0.7156\n",
            "Epoch 6: val_accuracy did not improve from 0.71678\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5715 - accuracy: 0.7156 - val_loss: 0.5653 - val_accuracy: 0.7161\n",
            "Epoch 7/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5703 - accuracy: 0.7168\n",
            "Epoch 7: val_accuracy improved from 0.71678 to 0.71878, saving model to classifier_weights2-improvement-07-0.72.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5703 - accuracy: 0.7168 - val_loss: 0.5613 - val_accuracy: 0.7188\n",
            "Epoch 8/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5684 - accuracy: 0.7172\n",
            "Epoch 8: val_accuracy improved from 0.71878 to 0.72023, saving model to classifier_weights2-improvement-08-0.72.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5684 - accuracy: 0.7173 - val_loss: 0.5596 - val_accuracy: 0.7202\n",
            "Epoch 9/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5683 - accuracy: 0.7174\n",
            "Epoch 9: val_accuracy improved from 0.72023 to 0.72090, saving model to classifier_weights2-improvement-09-0.72.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5682 - accuracy: 0.7175 - val_loss: 0.5600 - val_accuracy: 0.7209\n",
            "Epoch 10/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5653 - accuracy: 0.7206\n",
            "Epoch 10: val_accuracy improved from 0.72090 to 0.72364, saving model to classifier_weights2-improvement-10-0.72.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5653 - accuracy: 0.7205 - val_loss: 0.5570 - val_accuracy: 0.7236\n",
            "Epoch 11/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5640 - accuracy: 0.7210\n",
            "Epoch 11: val_accuracy did not improve from 0.72364\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5641 - accuracy: 0.7210 - val_loss: 0.5570 - val_accuracy: 0.7225\n",
            "Epoch 12/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5631 - accuracy: 0.7220\n",
            "Epoch 12: val_accuracy improved from 0.72364 to 0.72567, saving model to classifier_weights2-improvement-12-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5630 - accuracy: 0.7220 - val_loss: 0.5542 - val_accuracy: 0.7257\n",
            "Epoch 13/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5623 - accuracy: 0.7221\n",
            "Epoch 13: val_accuracy did not improve from 0.72567\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5623 - accuracy: 0.7221 - val_loss: 0.5574 - val_accuracy: 0.7245\n",
            "Epoch 14/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5610 - accuracy: 0.7230\n",
            "Epoch 14: val_accuracy improved from 0.72567 to 0.72646, saving model to classifier_weights2-improvement-14-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5610 - accuracy: 0.7230 - val_loss: 0.5535 - val_accuracy: 0.7265\n",
            "Epoch 15/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5606 - accuracy: 0.7227\n",
            "Epoch 15: val_accuracy improved from 0.72646 to 0.72678, saving model to classifier_weights2-improvement-15-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5607 - accuracy: 0.7227 - val_loss: 0.5520 - val_accuracy: 0.7268\n",
            "Epoch 16/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5600 - accuracy: 0.7239\n",
            "Epoch 16: val_accuracy did not improve from 0.72678\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5600 - accuracy: 0.7239 - val_loss: 0.5543 - val_accuracy: 0.7240\n",
            "Epoch 17/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5599 - accuracy: 0.7236\n",
            "Epoch 17: val_accuracy improved from 0.72678 to 0.72892, saving model to classifier_weights2-improvement-17-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5599 - accuracy: 0.7236 - val_loss: 0.5505 - val_accuracy: 0.7289\n",
            "Epoch 18/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.7244\n",
            "Epoch 18: val_accuracy did not improve from 0.72892\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5587 - accuracy: 0.7244 - val_loss: 0.5523 - val_accuracy: 0.7283\n",
            "Epoch 19/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5581 - accuracy: 0.7245\n",
            "Epoch 19: val_accuracy did not improve from 0.72892\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5581 - accuracy: 0.7245 - val_loss: 0.5537 - val_accuracy: 0.7251\n",
            "Epoch 20/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5577 - accuracy: 0.7257\n",
            "Epoch 20: val_accuracy improved from 0.72892 to 0.72951, saving model to classifier_weights2-improvement-20-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5577 - accuracy: 0.7257 - val_loss: 0.5488 - val_accuracy: 0.7295\n",
            "Epoch 21/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5559 - accuracy: 0.7267\n",
            "Epoch 21: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5559 - accuracy: 0.7266 - val_loss: 0.5486 - val_accuracy: 0.7288\n",
            "Epoch 22/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5550 - accuracy: 0.7278\n",
            "Epoch 22: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5549 - accuracy: 0.7278 - val_loss: 0.5504 - val_accuracy: 0.7279\n",
            "Epoch 23/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5557 - accuracy: 0.7276\n",
            "Epoch 23: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5558 - accuracy: 0.7275 - val_loss: 0.5504 - val_accuracy: 0.7290\n",
            "Epoch 24/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5550 - accuracy: 0.7278\n",
            "Epoch 24: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5550 - accuracy: 0.7278 - val_loss: 0.5498 - val_accuracy: 0.7275\n",
            "Epoch 25/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7277\n",
            "Epoch 25: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5545 - accuracy: 0.7277 - val_loss: 0.5498 - val_accuracy: 0.7291\n",
            "Epoch 26/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5540 - accuracy: 0.7275\n",
            "Epoch 26: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5540 - accuracy: 0.7275 - val_loss: 0.5495 - val_accuracy: 0.7286\n",
            "Epoch 27/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5524 - accuracy: 0.7293\n",
            "Epoch 27: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5525 - accuracy: 0.7292 - val_loss: 0.5492 - val_accuracy: 0.7292\n",
            "Epoch 28/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7298\n",
            "Epoch 28: val_accuracy did not improve from 0.72951\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5523 - accuracy: 0.7298 - val_loss: 0.5492 - val_accuracy: 0.7281\n",
            "Epoch 29/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7298\n",
            "Epoch 29: val_accuracy improved from 0.72951 to 0.73120, saving model to classifier_weights2-improvement-29-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5523 - accuracy: 0.7298 - val_loss: 0.5475 - val_accuracy: 0.7312\n",
            "Epoch 30/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5522 - accuracy: 0.7290\n",
            "Epoch 30: val_accuracy did not improve from 0.73120\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5520 - accuracy: 0.7291 - val_loss: 0.5482 - val_accuracy: 0.7302\n",
            "Epoch 31/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5523 - accuracy: 0.7288\n",
            "Epoch 31: val_accuracy did not improve from 0.73120\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5523 - accuracy: 0.7288 - val_loss: 0.5473 - val_accuracy: 0.7302\n",
            "Epoch 32/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7299\n",
            "Epoch 32: val_accuracy did not improve from 0.73120\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5508 - accuracy: 0.7299 - val_loss: 0.5472 - val_accuracy: 0.7301\n",
            "Epoch 33/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5507 - accuracy: 0.7302\n",
            "Epoch 33: val_accuracy improved from 0.73120 to 0.73208, saving model to classifier_weights2-improvement-33-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5508 - accuracy: 0.7301 - val_loss: 0.5460 - val_accuracy: 0.7321\n",
            "Epoch 34/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5506 - accuracy: 0.7301\n",
            "Epoch 34: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5505 - accuracy: 0.7302 - val_loss: 0.5476 - val_accuracy: 0.7299\n",
            "Epoch 35/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7317\n",
            "Epoch 35: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5491 - accuracy: 0.7316 - val_loss: 0.5471 - val_accuracy: 0.7299\n",
            "Epoch 36/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7315\n",
            "Epoch 36: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5492 - accuracy: 0.7315 - val_loss: 0.5477 - val_accuracy: 0.7299\n",
            "Epoch 37/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5490 - accuracy: 0.7312\n",
            "Epoch 37: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5490 - accuracy: 0.7313 - val_loss: 0.5490 - val_accuracy: 0.7286\n",
            "Epoch 38/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.7314\n",
            "Epoch 38: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5486 - accuracy: 0.7314 - val_loss: 0.5457 - val_accuracy: 0.7314\n",
            "Epoch 39/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5486 - accuracy: 0.7318\n",
            "Epoch 39: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5486 - accuracy: 0.7318 - val_loss: 0.5477 - val_accuracy: 0.7298\n",
            "Epoch 40/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5478 - accuracy: 0.7324\n",
            "Epoch 40: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5478 - accuracy: 0.7324 - val_loss: 0.5478 - val_accuracy: 0.7295\n",
            "Epoch 41/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5476 - accuracy: 0.7325\n",
            "Epoch 41: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5477 - accuracy: 0.7324 - val_loss: 0.5486 - val_accuracy: 0.7302\n",
            "Epoch 42/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.7323\n",
            "Epoch 42: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5480 - accuracy: 0.7323 - val_loss: 0.5496 - val_accuracy: 0.7269\n",
            "Epoch 43/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.7331\n",
            "Epoch 43: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5465 - accuracy: 0.7331 - val_loss: 0.5476 - val_accuracy: 0.7304\n",
            "Epoch 44/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5465 - accuracy: 0.7332\n",
            "Epoch 44: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5466 - accuracy: 0.7331 - val_loss: 0.5492 - val_accuracy: 0.7296\n",
            "Epoch 45/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5466 - accuracy: 0.7330\n",
            "Epoch 45: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5466 - accuracy: 0.7330 - val_loss: 0.5461 - val_accuracy: 0.7304\n",
            "Epoch 46/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5457 - accuracy: 0.7331\n",
            "Epoch 46: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5458 - accuracy: 0.7330 - val_loss: 0.5492 - val_accuracy: 0.7279\n",
            "Epoch 47/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5461 - accuracy: 0.7338\n",
            "Epoch 47: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5462 - accuracy: 0.7338 - val_loss: 0.5477 - val_accuracy: 0.7293\n",
            "Epoch 48/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5456 - accuracy: 0.7339\n",
            "Epoch 48: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5457 - accuracy: 0.7337 - val_loss: 0.5465 - val_accuracy: 0.7309\n",
            "Epoch 49/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5444 - accuracy: 0.7351\n",
            "Epoch 49: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5444 - accuracy: 0.7351 - val_loss: 0.5464 - val_accuracy: 0.7315\n",
            "Epoch 50/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5441 - accuracy: 0.7345\n",
            "Epoch 50: val_accuracy did not improve from 0.73208\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5442 - accuracy: 0.7346 - val_loss: 0.5462 - val_accuracy: 0.7317\n",
            "Epoch 51/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.7358\n",
            "Epoch 51: val_accuracy improved from 0.73208 to 0.73242, saving model to classifier_weights2-improvement-51-0.73.hdf5\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5440 - accuracy: 0.7358 - val_loss: 0.5443 - val_accuracy: 0.7324\n",
            "Epoch 52/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5439 - accuracy: 0.7351\n",
            "Epoch 52: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5439 - accuracy: 0.7353 - val_loss: 0.5469 - val_accuracy: 0.7306\n",
            "Epoch 53/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5433 - accuracy: 0.7355\n",
            "Epoch 53: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5433 - accuracy: 0.7355 - val_loss: 0.5452 - val_accuracy: 0.7311\n",
            "Epoch 54/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5430 - accuracy: 0.7359\n",
            "Epoch 54: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5430 - accuracy: 0.7360 - val_loss: 0.5459 - val_accuracy: 0.7308\n",
            "Epoch 55/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7352\n",
            "Epoch 55: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5433 - accuracy: 0.7351 - val_loss: 0.5455 - val_accuracy: 0.7311\n",
            "Epoch 56/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5425 - accuracy: 0.7354\n",
            "Epoch 56: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5424 - accuracy: 0.7354 - val_loss: 0.5456 - val_accuracy: 0.7311\n",
            "Epoch 57/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5427 - accuracy: 0.7359\n",
            "Epoch 57: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5427 - accuracy: 0.7359 - val_loss: 0.5484 - val_accuracy: 0.7296\n",
            "Epoch 58/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5423 - accuracy: 0.7361\n",
            "Epoch 58: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5422 - accuracy: 0.7362 - val_loss: 0.5508 - val_accuracy: 0.7265\n",
            "Epoch 59/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5418 - accuracy: 0.7362\n",
            "Epoch 59: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5417 - accuracy: 0.7363 - val_loss: 0.5464 - val_accuracy: 0.7311\n",
            "Epoch 60/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5411 - accuracy: 0.7375\n",
            "Epoch 60: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5411 - accuracy: 0.7375 - val_loss: 0.5467 - val_accuracy: 0.7295\n",
            "Epoch 61/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5407 - accuracy: 0.7377\n",
            "Epoch 61: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5407 - accuracy: 0.7377 - val_loss: 0.5456 - val_accuracy: 0.7321\n",
            "Epoch 62/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7384\n",
            "Epoch 62: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5396 - accuracy: 0.7384 - val_loss: 0.5473 - val_accuracy: 0.7297\n",
            "Epoch 63/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5406 - accuracy: 0.7376\n",
            "Epoch 63: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5405 - accuracy: 0.7376 - val_loss: 0.5474 - val_accuracy: 0.7293\n",
            "Epoch 64/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.7378\n",
            "Epoch 64: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5404 - accuracy: 0.7378 - val_loss: 0.5476 - val_accuracy: 0.7290\n",
            "Epoch 65/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5401 - accuracy: 0.7382\n",
            "Epoch 65: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5401 - accuracy: 0.7382 - val_loss: 0.5465 - val_accuracy: 0.7304\n",
            "Epoch 66/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7386\n",
            "Epoch 66: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5392 - accuracy: 0.7387 - val_loss: 0.5464 - val_accuracy: 0.7304\n",
            "Epoch 67/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.7379\n",
            "Epoch 67: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5387 - accuracy: 0.7379 - val_loss: 0.5458 - val_accuracy: 0.7313\n",
            "Epoch 68/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5392 - accuracy: 0.7386\n",
            "Epoch 68: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5392 - accuracy: 0.7386 - val_loss: 0.5468 - val_accuracy: 0.7289\n",
            "Epoch 69/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5396 - accuracy: 0.7377\n",
            "Epoch 69: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5396 - accuracy: 0.7377 - val_loss: 0.5457 - val_accuracy: 0.7311\n",
            "Epoch 70/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5378 - accuracy: 0.7390\n",
            "Epoch 70: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5378 - accuracy: 0.7389 - val_loss: 0.5478 - val_accuracy: 0.7289\n",
            "Epoch 71/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7392\n",
            "Epoch 71: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5380 - accuracy: 0.7392 - val_loss: 0.5471 - val_accuracy: 0.7301\n",
            "Epoch 72/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5375 - accuracy: 0.7398\n",
            "Epoch 72: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5375 - accuracy: 0.7399 - val_loss: 0.5465 - val_accuracy: 0.7304\n",
            "Epoch 73/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5376 - accuracy: 0.7402\n",
            "Epoch 73: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5376 - accuracy: 0.7401 - val_loss: 0.5479 - val_accuracy: 0.7281\n",
            "Epoch 74/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5375 - accuracy: 0.7399\n",
            "Epoch 74: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5374 - accuracy: 0.7399 - val_loss: 0.5477 - val_accuracy: 0.7305\n",
            "Epoch 75/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7403\n",
            "Epoch 75: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5362 - accuracy: 0.7403 - val_loss: 0.5450 - val_accuracy: 0.7321\n",
            "Epoch 76/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5361 - accuracy: 0.7408\n",
            "Epoch 76: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5361 - accuracy: 0.7409 - val_loss: 0.5511 - val_accuracy: 0.7262\n",
            "Epoch 77/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.7403\n",
            "Epoch 77: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5366 - accuracy: 0.7403 - val_loss: 0.5482 - val_accuracy: 0.7298\n",
            "Epoch 78/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5355 - accuracy: 0.7410\n",
            "Epoch 78: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5355 - accuracy: 0.7410 - val_loss: 0.5466 - val_accuracy: 0.7316\n",
            "Epoch 79/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7418\n",
            "Epoch 79: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5346 - accuracy: 0.7418 - val_loss: 0.5465 - val_accuracy: 0.7311\n",
            "Epoch 80/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.7414\n",
            "Epoch 80: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5352 - accuracy: 0.7414 - val_loss: 0.5478 - val_accuracy: 0.7294\n",
            "Epoch 81/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5347 - accuracy: 0.7421\n",
            "Epoch 81: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5344 - accuracy: 0.7422 - val_loss: 0.5472 - val_accuracy: 0.7308\n",
            "Epoch 82/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7414\n",
            "Epoch 82: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5349 - accuracy: 0.7414 - val_loss: 0.5464 - val_accuracy: 0.7305\n",
            "Epoch 83/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7421\n",
            "Epoch 83: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5348 - accuracy: 0.7423 - val_loss: 0.5460 - val_accuracy: 0.7302\n",
            "Epoch 84/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5344 - accuracy: 0.7422\n",
            "Epoch 84: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5344 - accuracy: 0.7422 - val_loss: 0.5480 - val_accuracy: 0.7299\n",
            "Epoch 85/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5337 - accuracy: 0.7426\n",
            "Epoch 85: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5334 - accuracy: 0.7428 - val_loss: 0.5463 - val_accuracy: 0.7322\n",
            "Epoch 86/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5337 - accuracy: 0.7414\n",
            "Epoch 86: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5338 - accuracy: 0.7413 - val_loss: 0.5451 - val_accuracy: 0.7321\n",
            "Epoch 87/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5334 - accuracy: 0.7424\n",
            "Epoch 87: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5334 - accuracy: 0.7424 - val_loss: 0.5470 - val_accuracy: 0.7305\n",
            "Epoch 88/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5320 - accuracy: 0.7435\n",
            "Epoch 88: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5321 - accuracy: 0.7435 - val_loss: 0.5468 - val_accuracy: 0.7300\n",
            "Epoch 89/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5326 - accuracy: 0.7431\n",
            "Epoch 89: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5326 - accuracy: 0.7430 - val_loss: 0.5460 - val_accuracy: 0.7310\n",
            "Epoch 90/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5321 - accuracy: 0.7430\n",
            "Epoch 90: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5321 - accuracy: 0.7429 - val_loss: 0.5483 - val_accuracy: 0.7272\n",
            "Epoch 91/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5320 - accuracy: 0.7433\n",
            "Epoch 91: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5320 - accuracy: 0.7433 - val_loss: 0.5473 - val_accuracy: 0.7295\n",
            "Epoch 92/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5308 - accuracy: 0.7443\n",
            "Epoch 92: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5308 - accuracy: 0.7443 - val_loss: 0.5477 - val_accuracy: 0.7308\n",
            "Epoch 93/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5312 - accuracy: 0.7442\n",
            "Epoch 93: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5313 - accuracy: 0.7441 - val_loss: 0.5482 - val_accuracy: 0.7304\n",
            "Epoch 94/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5308 - accuracy: 0.7440\n",
            "Epoch 94: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5308 - accuracy: 0.7440 - val_loss: 0.5489 - val_accuracy: 0.7293\n",
            "Epoch 95/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5302 - accuracy: 0.7446\n",
            "Epoch 95: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5302 - accuracy: 0.7446 - val_loss: 0.5475 - val_accuracy: 0.7292\n",
            "Epoch 96/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5299 - accuracy: 0.7446\n",
            "Epoch 96: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5299 - accuracy: 0.7446 - val_loss: 0.5469 - val_accuracy: 0.7304\n",
            "Epoch 97/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5300 - accuracy: 0.7452\n",
            "Epoch 97: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5297 - accuracy: 0.7454 - val_loss: 0.5479 - val_accuracy: 0.7300\n",
            "Epoch 98/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5300 - accuracy: 0.7453\n",
            "Epoch 98: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5300 - accuracy: 0.7452 - val_loss: 0.5488 - val_accuracy: 0.7303\n",
            "Epoch 99/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5295 - accuracy: 0.7447\n",
            "Epoch 99: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5293 - accuracy: 0.7449 - val_loss: 0.5476 - val_accuracy: 0.7311\n",
            "Epoch 100/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5304 - accuracy: 0.7443\n",
            "Epoch 100: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5304 - accuracy: 0.7443 - val_loss: 0.5487 - val_accuracy: 0.7287\n",
            "Epoch 101/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5288 - accuracy: 0.7461\n",
            "Epoch 101: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5288 - accuracy: 0.7460 - val_loss: 0.5490 - val_accuracy: 0.7292\n",
            "Epoch 102/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5285 - accuracy: 0.7455\n",
            "Epoch 102: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5285 - accuracy: 0.7455 - val_loss: 0.5478 - val_accuracy: 0.7314\n",
            "Epoch 103/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5294 - accuracy: 0.7457\n",
            "Epoch 103: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5294 - accuracy: 0.7458 - val_loss: 0.5484 - val_accuracy: 0.7297\n",
            "Epoch 104/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5285 - accuracy: 0.7457\n",
            "Epoch 104: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5285 - accuracy: 0.7457 - val_loss: 0.5478 - val_accuracy: 0.7301\n",
            "Epoch 105/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5276 - accuracy: 0.7470\n",
            "Epoch 105: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5276 - accuracy: 0.7470 - val_loss: 0.5473 - val_accuracy: 0.7307\n",
            "Epoch 106/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5268 - accuracy: 0.7465\n",
            "Epoch 106: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5268 - accuracy: 0.7465 - val_loss: 0.5495 - val_accuracy: 0.7303\n",
            "Epoch 107/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5267 - accuracy: 0.7471\n",
            "Epoch 107: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5267 - accuracy: 0.7472 - val_loss: 0.5489 - val_accuracy: 0.7303\n",
            "Epoch 108/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.7464\n",
            "Epoch 108: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5272 - accuracy: 0.7465 - val_loss: 0.5498 - val_accuracy: 0.7297\n",
            "Epoch 109/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.7479\n",
            "Epoch 109: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5262 - accuracy: 0.7480 - val_loss: 0.5481 - val_accuracy: 0.7305\n",
            "Epoch 110/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5260 - accuracy: 0.7472\n",
            "Epoch 110: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5261 - accuracy: 0.7470 - val_loss: 0.5488 - val_accuracy: 0.7285\n",
            "Epoch 111/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7485\n",
            "Epoch 111: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5251 - accuracy: 0.7485 - val_loss: 0.5514 - val_accuracy: 0.7297\n",
            "Epoch 112/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5245 - accuracy: 0.7487\n",
            "Epoch 112: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5245 - accuracy: 0.7487 - val_loss: 0.5491 - val_accuracy: 0.7298\n",
            "Epoch 113/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5256 - accuracy: 0.7479\n",
            "Epoch 113: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5256 - accuracy: 0.7479 - val_loss: 0.5493 - val_accuracy: 0.7307\n",
            "Epoch 114/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5258 - accuracy: 0.7468\n",
            "Epoch 114: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5258 - accuracy: 0.7469 - val_loss: 0.5512 - val_accuracy: 0.7278\n",
            "Epoch 115/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5242 - accuracy: 0.7487\n",
            "Epoch 115: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5242 - accuracy: 0.7487 - val_loss: 0.5489 - val_accuracy: 0.7295\n",
            "Epoch 116/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5240 - accuracy: 0.7495\n",
            "Epoch 116: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5239 - accuracy: 0.7496 - val_loss: 0.5496 - val_accuracy: 0.7295\n",
            "Epoch 117/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5247 - accuracy: 0.7485\n",
            "Epoch 117: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5247 - accuracy: 0.7485 - val_loss: 0.5505 - val_accuracy: 0.7301\n",
            "Epoch 118/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5238 - accuracy: 0.7486\n",
            "Epoch 118: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5238 - accuracy: 0.7486 - val_loss: 0.5511 - val_accuracy: 0.7277\n",
            "Epoch 119/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5247 - accuracy: 0.7490\n",
            "Epoch 119: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5247 - accuracy: 0.7490 - val_loss: 0.5496 - val_accuracy: 0.7302\n",
            "Epoch 120/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.7490\n",
            "Epoch 120: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5232 - accuracy: 0.7490 - val_loss: 0.5510 - val_accuracy: 0.7291\n",
            "Epoch 121/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5231 - accuracy: 0.7492\n",
            "Epoch 121: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5230 - accuracy: 0.7492 - val_loss: 0.5492 - val_accuracy: 0.7302\n",
            "Epoch 122/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5228 - accuracy: 0.7497\n",
            "Epoch 122: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5228 - accuracy: 0.7497 - val_loss: 0.5492 - val_accuracy: 0.7303\n",
            "Epoch 123/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5230 - accuracy: 0.7506\n",
            "Epoch 123: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5230 - accuracy: 0.7507 - val_loss: 0.5493 - val_accuracy: 0.7298\n",
            "Epoch 124/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5224 - accuracy: 0.7501\n",
            "Epoch 124: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5224 - accuracy: 0.7501 - val_loss: 0.5509 - val_accuracy: 0.7298\n",
            "Epoch 125/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.7501\n",
            "Epoch 125: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5222 - accuracy: 0.7501 - val_loss: 0.5489 - val_accuracy: 0.7295\n",
            "Epoch 126/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5220 - accuracy: 0.7494\n",
            "Epoch 126: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5219 - accuracy: 0.7495 - val_loss: 0.5501 - val_accuracy: 0.7298\n",
            "Epoch 127/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5218 - accuracy: 0.7505\n",
            "Epoch 127: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5218 - accuracy: 0.7505 - val_loss: 0.5492 - val_accuracy: 0.7299\n",
            "Epoch 128/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5222 - accuracy: 0.7504\n",
            "Epoch 128: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5223 - accuracy: 0.7503 - val_loss: 0.5519 - val_accuracy: 0.7268\n",
            "Epoch 129/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5215 - accuracy: 0.7500\n",
            "Epoch 129: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5215 - accuracy: 0.7502 - val_loss: 0.5515 - val_accuracy: 0.7284\n",
            "Epoch 130/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5212 - accuracy: 0.7510\n",
            "Epoch 130: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5212 - accuracy: 0.7510 - val_loss: 0.5513 - val_accuracy: 0.7292\n",
            "Epoch 131/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5202 - accuracy: 0.7517\n",
            "Epoch 131: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5202 - accuracy: 0.7517 - val_loss: 0.5518 - val_accuracy: 0.7289\n",
            "Epoch 132/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.7520\n",
            "Epoch 132: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5203 - accuracy: 0.7518 - val_loss: 0.5510 - val_accuracy: 0.7301\n",
            "Epoch 133/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5198 - accuracy: 0.7519\n",
            "Epoch 133: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5198 - accuracy: 0.7519 - val_loss: 0.5506 - val_accuracy: 0.7289\n",
            "Epoch 134/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5203 - accuracy: 0.7518\n",
            "Epoch 134: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5203 - accuracy: 0.7517 - val_loss: 0.5508 - val_accuracy: 0.7276\n",
            "Epoch 135/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5194 - accuracy: 0.7524\n",
            "Epoch 135: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5195 - accuracy: 0.7523 - val_loss: 0.5497 - val_accuracy: 0.7296\n",
            "Epoch 136/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5188 - accuracy: 0.7528\n",
            "Epoch 136: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5187 - accuracy: 0.7528 - val_loss: 0.5521 - val_accuracy: 0.7280\n",
            "Epoch 137/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5190 - accuracy: 0.7515\n",
            "Epoch 137: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5190 - accuracy: 0.7515 - val_loss: 0.5513 - val_accuracy: 0.7296\n",
            "Epoch 138/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5183 - accuracy: 0.7525\n",
            "Epoch 138: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5183 - accuracy: 0.7526 - val_loss: 0.5526 - val_accuracy: 0.7294\n",
            "Epoch 139/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5184 - accuracy: 0.7521\n",
            "Epoch 139: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5183 - accuracy: 0.7521 - val_loss: 0.5502 - val_accuracy: 0.7301\n",
            "Epoch 140/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5172 - accuracy: 0.7537\n",
            "Epoch 140: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5174 - accuracy: 0.7535 - val_loss: 0.5529 - val_accuracy: 0.7280\n",
            "Epoch 141/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5178 - accuracy: 0.7530\n",
            "Epoch 141: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5178 - accuracy: 0.7529 - val_loss: 0.5520 - val_accuracy: 0.7285\n",
            "Epoch 142/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5189 - accuracy: 0.7515\n",
            "Epoch 142: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5189 - accuracy: 0.7516 - val_loss: 0.5549 - val_accuracy: 0.7293\n",
            "Epoch 143/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5175 - accuracy: 0.7533\n",
            "Epoch 143: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5174 - accuracy: 0.7535 - val_loss: 0.5526 - val_accuracy: 0.7292\n",
            "Epoch 144/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5182 - accuracy: 0.7528\n",
            "Epoch 144: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5182 - accuracy: 0.7527 - val_loss: 0.5536 - val_accuracy: 0.7287\n",
            "Epoch 145/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5162 - accuracy: 0.7540\n",
            "Epoch 145: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5162 - accuracy: 0.7540 - val_loss: 0.5524 - val_accuracy: 0.7298\n",
            "Epoch 146/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5158 - accuracy: 0.7537\n",
            "Epoch 146: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5158 - accuracy: 0.7536 - val_loss: 0.5537 - val_accuracy: 0.7297\n",
            "Epoch 147/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5170 - accuracy: 0.7531\n",
            "Epoch 147: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5170 - accuracy: 0.7532 - val_loss: 0.5540 - val_accuracy: 0.7291\n",
            "Epoch 148/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5172 - accuracy: 0.7522\n",
            "Epoch 148: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5173 - accuracy: 0.7522 - val_loss: 0.5530 - val_accuracy: 0.7280\n",
            "Epoch 149/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5160 - accuracy: 0.7533\n",
            "Epoch 149: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5160 - accuracy: 0.7533 - val_loss: 0.5526 - val_accuracy: 0.7301\n",
            "Epoch 150/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.7543\n",
            "Epoch 150: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5148 - accuracy: 0.7544 - val_loss: 0.5543 - val_accuracy: 0.7285\n",
            "Epoch 151/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5150 - accuracy: 0.7552\n",
            "Epoch 151: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5150 - accuracy: 0.7552 - val_loss: 0.5556 - val_accuracy: 0.7274\n",
            "Epoch 152/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5150 - accuracy: 0.7544\n",
            "Epoch 152: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5150 - accuracy: 0.7544 - val_loss: 0.5571 - val_accuracy: 0.7277\n",
            "Epoch 153/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5142 - accuracy: 0.7556\n",
            "Epoch 153: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5142 - accuracy: 0.7556 - val_loss: 0.5531 - val_accuracy: 0.7299\n",
            "Epoch 154/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5141 - accuracy: 0.7556\n",
            "Epoch 154: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5142 - accuracy: 0.7556 - val_loss: 0.5538 - val_accuracy: 0.7286\n",
            "Epoch 155/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5144 - accuracy: 0.7551\n",
            "Epoch 155: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 19ms/step - loss: 0.5145 - accuracy: 0.7550 - val_loss: 0.5543 - val_accuracy: 0.7281\n",
            "Epoch 156/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7559\n",
            "Epoch 156: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5135 - accuracy: 0.7558 - val_loss: 0.5542 - val_accuracy: 0.7278\n",
            "Epoch 157/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5129 - accuracy: 0.7557\n",
            "Epoch 157: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5129 - accuracy: 0.7557 - val_loss: 0.5557 - val_accuracy: 0.7278\n",
            "Epoch 158/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5138 - accuracy: 0.7548\n",
            "Epoch 158: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5138 - accuracy: 0.7549 - val_loss: 0.5539 - val_accuracy: 0.7278\n",
            "Epoch 159/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5129 - accuracy: 0.7557\n",
            "Epoch 159: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5129 - accuracy: 0.7557 - val_loss: 0.5520 - val_accuracy: 0.7300\n",
            "Epoch 160/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5116 - accuracy: 0.7568\n",
            "Epoch 160: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5117 - accuracy: 0.7567 - val_loss: 0.5536 - val_accuracy: 0.7301\n",
            "Epoch 161/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5116 - accuracy: 0.7570\n",
            "Epoch 161: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5116 - accuracy: 0.7571 - val_loss: 0.5569 - val_accuracy: 0.7284\n",
            "Epoch 162/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.7559\n",
            "Epoch 162: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5134 - accuracy: 0.7559 - val_loss: 0.5546 - val_accuracy: 0.7273\n",
            "Epoch 163/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5113 - accuracy: 0.7574\n",
            "Epoch 163: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5111 - accuracy: 0.7575 - val_loss: 0.5574 - val_accuracy: 0.7285\n",
            "Epoch 164/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5111 - accuracy: 0.7576\n",
            "Epoch 164: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5111 - accuracy: 0.7576 - val_loss: 0.5565 - val_accuracy: 0.7279\n",
            "Epoch 165/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5118 - accuracy: 0.7567\n",
            "Epoch 165: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5117 - accuracy: 0.7568 - val_loss: 0.5568 - val_accuracy: 0.7281\n",
            "Epoch 166/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5107 - accuracy: 0.7577\n",
            "Epoch 166: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5106 - accuracy: 0.7578 - val_loss: 0.5577 - val_accuracy: 0.7287\n",
            "Epoch 167/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.7572\n",
            "Epoch 167: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5114 - accuracy: 0.7572 - val_loss: 0.5549 - val_accuracy: 0.7292\n",
            "Epoch 168/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5111 - accuracy: 0.7570\n",
            "Epoch 168: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5112 - accuracy: 0.7570 - val_loss: 0.5560 - val_accuracy: 0.7289\n",
            "Epoch 169/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.7570\n",
            "Epoch 169: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5109 - accuracy: 0.7570 - val_loss: 0.5593 - val_accuracy: 0.7248\n",
            "Epoch 170/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5110 - accuracy: 0.7568\n",
            "Epoch 170: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5110 - accuracy: 0.7569 - val_loss: 0.5569 - val_accuracy: 0.7279\n",
            "Epoch 171/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5104 - accuracy: 0.7578\n",
            "Epoch 171: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5103 - accuracy: 0.7578 - val_loss: 0.5570 - val_accuracy: 0.7275\n",
            "Epoch 172/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5101 - accuracy: 0.7574\n",
            "Epoch 172: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5102 - accuracy: 0.7573 - val_loss: 0.5569 - val_accuracy: 0.7268\n",
            "Epoch 173/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.7570\n",
            "Epoch 173: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5100 - accuracy: 0.7570 - val_loss: 0.5586 - val_accuracy: 0.7295\n",
            "Epoch 174/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5099 - accuracy: 0.7585\n",
            "Epoch 174: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5098 - accuracy: 0.7585 - val_loss: 0.5611 - val_accuracy: 0.7276\n",
            "Epoch 175/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5083 - accuracy: 0.7594\n",
            "Epoch 175: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5084 - accuracy: 0.7593 - val_loss: 0.5612 - val_accuracy: 0.7258\n",
            "Epoch 176/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5094 - accuracy: 0.7580\n",
            "Epoch 176: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5094 - accuracy: 0.7580 - val_loss: 0.5573 - val_accuracy: 0.7282\n",
            "Epoch 177/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5085 - accuracy: 0.7588\n",
            "Epoch 177: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5085 - accuracy: 0.7587 - val_loss: 0.5602 - val_accuracy: 0.7272\n",
            "Epoch 178/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5077 - accuracy: 0.7592\n",
            "Epoch 178: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5077 - accuracy: 0.7591 - val_loss: 0.5579 - val_accuracy: 0.7284\n",
            "Epoch 179/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5091 - accuracy: 0.7582\n",
            "Epoch 179: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5090 - accuracy: 0.7583 - val_loss: 0.5582 - val_accuracy: 0.7272\n",
            "Epoch 180/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.7587\n",
            "Epoch 180: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5084 - accuracy: 0.7587 - val_loss: 0.5575 - val_accuracy: 0.7268\n",
            "Epoch 181/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5072 - accuracy: 0.7597\n",
            "Epoch 181: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5073 - accuracy: 0.7596 - val_loss: 0.5598 - val_accuracy: 0.7259\n",
            "Epoch 182/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5076 - accuracy: 0.7589\n",
            "Epoch 182: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 16ms/step - loss: 0.5076 - accuracy: 0.7589 - val_loss: 0.5610 - val_accuracy: 0.7276\n",
            "Epoch 183/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5072 - accuracy: 0.7596\n",
            "Epoch 183: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5073 - accuracy: 0.7596 - val_loss: 0.5589 - val_accuracy: 0.7260\n",
            "Epoch 184/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5069 - accuracy: 0.7606\n",
            "Epoch 184: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.5070 - accuracy: 0.7606 - val_loss: 0.5599 - val_accuracy: 0.7268\n",
            "Epoch 185/200\n",
            "157/160 [============================>.] - ETA: 0s - loss: 0.5074 - accuracy: 0.7589\n",
            "Epoch 185: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5073 - accuracy: 0.7589 - val_loss: 0.5613 - val_accuracy: 0.7286\n",
            "Epoch 186/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5067 - accuracy: 0.7595\n",
            "Epoch 186: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.5067 - accuracy: 0.7595 - val_loss: 0.5631 - val_accuracy: 0.7273\n",
            "Epoch 187/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5067 - accuracy: 0.7600\n",
            "Epoch 187: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5067 - accuracy: 0.7599 - val_loss: 0.5602 - val_accuracy: 0.7269\n",
            "Epoch 188/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5059 - accuracy: 0.7605\n",
            "Epoch 188: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 22ms/step - loss: 0.5060 - accuracy: 0.7604 - val_loss: 0.5598 - val_accuracy: 0.7259\n",
            "Epoch 189/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5050 - accuracy: 0.7613\n",
            "Epoch 189: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 24ms/step - loss: 0.5052 - accuracy: 0.7612 - val_loss: 0.5600 - val_accuracy: 0.7272\n",
            "Epoch 190/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5060 - accuracy: 0.7603\n",
            "Epoch 190: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 21ms/step - loss: 0.5061 - accuracy: 0.7603 - val_loss: 0.5630 - val_accuracy: 0.7265\n",
            "Epoch 191/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5059 - accuracy: 0.7609\n",
            "Epoch 191: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.5060 - accuracy: 0.7609 - val_loss: 0.5613 - val_accuracy: 0.7273\n",
            "Epoch 192/200\n",
            "159/160 [============================>.] - ETA: 0s - loss: 0.5061 - accuracy: 0.7605\n",
            "Epoch 192: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 20ms/step - loss: 0.5061 - accuracy: 0.7605 - val_loss: 0.5614 - val_accuracy: 0.7284\n",
            "Epoch 193/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5062 - accuracy: 0.7613\n",
            "Epoch 193: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5063 - accuracy: 0.7612 - val_loss: 0.5637 - val_accuracy: 0.7270\n",
            "Epoch 194/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5044 - accuracy: 0.7617\n",
            "Epoch 194: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 23ms/step - loss: 0.5042 - accuracy: 0.7619 - val_loss: 0.5636 - val_accuracy: 0.7259\n",
            "Epoch 195/200\n",
            "160/160 [==============================] - ETA: 0s - loss: 0.5040 - accuracy: 0.7615\n",
            "Epoch 195: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 4s 22ms/step - loss: 0.5040 - accuracy: 0.7615 - val_loss: 0.5605 - val_accuracy: 0.7284\n",
            "Epoch 196/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.7616\n",
            "Epoch 196: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 18ms/step - loss: 0.5048 - accuracy: 0.7615 - val_loss: 0.5619 - val_accuracy: 0.7288\n",
            "Epoch 197/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5039 - accuracy: 0.7611\n",
            "Epoch 197: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5038 - accuracy: 0.7611 - val_loss: 0.5635 - val_accuracy: 0.7256\n",
            "Epoch 198/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5040 - accuracy: 0.7611\n",
            "Epoch 198: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5041 - accuracy: 0.7609 - val_loss: 0.5611 - val_accuracy: 0.7279\n",
            "Epoch 199/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5046 - accuracy: 0.7610\n",
            "Epoch 199: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5046 - accuracy: 0.7609 - val_loss: 0.5612 - val_accuracy: 0.7267\n",
            "Epoch 200/200\n",
            "158/160 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.7609\n",
            "Epoch 200: val_accuracy did not improve from 0.73242\n",
            "160/160 [==============================] - 3s 17ms/step - loss: 0.5047 - accuracy: 0.7609 - val_loss: 0.5623 - val_accuracy: 0.7273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the results:"
      ],
      "metadata": {
        "id": "VLaz0XRbhHxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Accuracy of the model')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train', 'Test'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss metrics of the model')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6W9AXuCDhKPB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "836fbd4d-e439-4f49-dc21-85492139bc32"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the performance of the model on predictions:"
      ],
      "metadata": {
        "id": "zuK6QGdyhWip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_epoch=np.argmax(history.history['val_accuracy'])\n",
        "best_acc=np.max(history.history['val_accuracy'])\n",
        "model.load_weights(f\"classifier_weights2-improvement-{best_epoch+1}-{best_acc:.2f}.hdf5\")\n",
        "predictions = model.predict(X_train)\n",
        "bin =[0 if p<0.5 else 1 for p in predictions]"
      ],
      "metadata": {
        "id": "dm2VGlYAiucb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P8Y71U6FixE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_train,bin))\n",
        "roc_auc_score(y_train, bin)"
      ],
      "metadata": {
        "id": "sWfZcWBwjPje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3a2d3c-0b07-495a-b5f2-4991f9f88a28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.75      0.72      0.74    199124\n",
            "         1.0       0.73      0.76      0.75    199276\n",
            "\n",
            "    accuracy                           0.74    398400\n",
            "   macro avg       0.74      0.74      0.74    398400\n",
            "weighted avg       0.74      0.74      0.74    398400\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.741123700089779"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References :**\n",
        "\n",
        "\n",
        "1.   [Examining Electron and Photon Classification Using Convolutional Neural Networks\n",
        "Jonah Warner, Research Assistant\n",
        "Department of Physics, Carnegie Mellon University, Pittsburgh 15213](https://www.cmu.edu/ai-physics-institute/outreach/surp/images/2021/jonah-warner-poster.pdf)\n",
        "2.   [End-to-End Event Classification of High-Energy\n",
        "Physics Data\n",
        "M Andrews\n",
        ", M Paulini\n",
        ", S Gleyzer\n",
        ", B Poczos](https://indico.cern.ch/event/567550/papers/2629451/files/7515-end-end-event_v4.pdf)\n",
        "3.   [Calorimetry with Deep Learning: Particle\n",
        "Classification, Energy Regression, and Simulation for\n",
        "High-Energy Physics\n",
        "Federico Carminati, Gulrukh Khattak, Maurizio Pierini\n",
        "CERN](https://dl4physicalsciences.github.io/files/nips_dlps_2017_15.pdf)\n",
        "4.  [Electron/Photon Ambiguity Resolution Using Neural\n",
        "networks For ATLAS Experiment\n",
        "Nutthawara Buatthaisong, Khon Kaen University, Thailand\n",
        "](https://www.desy.de/f/students/2019/reports/nutthawara.buatthaisong.pdf)\n"
      ],
      "metadata": {
        "id": "Z1wnCtwaPVGx"
      }
    }
  ]
}