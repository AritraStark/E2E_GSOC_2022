{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E2E_eval_task_1_PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzO9T1uvX2PvIlXuIZtYqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AritraStark/E2E_GSOC_2022/blob/main/E2E_eval_task_1_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Task 1. Electron/photon classification**\n",
        "\n",
        "**Datasets:**\n",
        "\n",
        "https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc (photons)\n",
        "\n",
        "https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA (electrons)\n",
        "\n",
        "**Description:** 32x32 matrices (two channels - hit energy and time) for two classes of particles electrons and photons impinging on a calorimeter Please use a deep learning method of your choice to achieve the highest possible classification on this dataset (we ask that you do it both in Keras/Tensorflow and in PyTorch). Please provide a Jupyter notebook that shows your solution. The model yousubmit should have a ROC AUC score of at least 0.80.\n",
        "\n"
      ],
      "metadata": {
        "id": "8kSM2bM5Rl1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the dataset: "
      ],
      "metadata": {
        "id": "yB0RibT5R7Tx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emFLK8-cRcYx"
      },
      "outputs": [],
      "source": [
        "!wget https://cernbox.cern.ch/index.php/s/AtBT8y4MiQYFcgc/download -O photons.hdf5\n",
        "!wget https://cernbox.cern.ch/index.php/s/FbXw3V4XNyYB3oA/download -O electrons.hdf5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "particles_x_train,particles_x_val,particles_y_train,particles_y_val=train_test_split(particles_x_train,particles_y_train,random_state=48,test_size=0.2)"
      ],
      "metadata": {
        "id": "CyY260tmTGHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d0IejaPtSAFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "from torch import Tensor\n",
        "\n",
        "#first load the data into tensor datasets\n",
        "train_dataset=TensorDataset(Tensor(particles_x_train),Tensor(particles_y_train))\n",
        "val_dataset=TensorDataset(Tensor(particles_x_val),Tensor(particles_y_val))\n",
        "test_dataset=TensorDataset(Tensor(particles_x_test),Tensor(particles_y_test))\n",
        "\n",
        "#next load the tensor datasets into Dataloaders and make sure to activate the shuffle feature\n",
        "trainloader=DataLoader(train_dataset,shuffle=True,batch_size=2000)\n",
        "validloader=DataLoader(val_dataset,shuffle=True,batch_size=2000)\n",
        "testloader=DataLoader(test_dataset,shuffle=True,batch_size=2000)"
      ],
      "metadata": {
        "id": "DtF7HnsWSGEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Network(nn.Module):\n",
        "## constructor definition\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1=nn.Linear(32*32,256)\n",
        "    self.fc2=nn.Linear(256,256)\n",
        "    self.fc3=nn.Linear(256,256)\n",
        "    self.fc4=nn.Linear(256,256)\n",
        "    self.fc5=nn.Linear(256,1)\n",
        "    self.dropout=nn.Dropout(0.5)\n",
        "\n",
        "## forward method definition\n",
        "  def forward(self,x):\n",
        "    x=x.view(x.shape[0],-1)\n",
        "    x=F.relu(self.fc1(x))\n",
        "    x=self.dropout(x)\n",
        "    x=F.relu(self.fc2(x))\n",
        "    x=self.dropout(x)\n",
        "    x=F.relu(self.fc3(x))\n",
        "    x=self.dropout(x)\n",
        "    x=F.relu(self.fc4(x)) \n",
        "    x=self.dropout(x)   \n",
        "    x=self.fc5(x).reshape(-1)\n",
        "    return x\n",
        "\n",
        "#now we can instantiate the Neural Network\n",
        "model=Network().cuda()"
      ],
      "metadata": {
        "id": "-Ryu9UlMSG-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCEWithLogitsLoss().cuda()"
      ],
      "metadata": {
        "id": "EcVT4b3hSPGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "lwhfksTiSQLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of epochs to train model for\n",
        "epochs=100\n",
        "\n",
        "#keep track of the train and validation losses over each epoch\n",
        "train_losses=[]\n",
        "val_losses=[]\n",
        "\n",
        "#keep track of the lowest validation loss\n",
        "min_val_loss=np.inf\n",
        "\n",
        "#keep track of the epoch with the lowest validation loss\n",
        "best_epoch=0"
      ],
      "metadata": {
        "id": "zIdnfMiuSbha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loop over number of epochs\n",
        "for i in range(epochs):\n",
        "  train_loss=0\n",
        "  val_loss=0\n",
        "  #set model to train mode\n",
        "  model.train()\n",
        "  for data,labels in trainloader:\n",
        "    \n",
        "    #zeroing the gradients saved with the optimizer\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #forward propagation\n",
        "    probs=model(data.cuda())   \n",
        "    #loss evaluation\n",
        "    loss=criterion(probs,labels.cuda())\n",
        "    \n",
        "    #backward propagation\n",
        "    loss.backward()\n",
        "    \n",
        "    #updating weights\n",
        "    optimizer.step()\n",
        "    \n",
        "    #accumulating training loss over epoch\n",
        "    train_loss+= loss.item()* data.shape[0]\n",
        "\n",
        "  #set model to evaluation mode\n",
        "  model.eval()\n",
        "  for data,labels in validloader:\n",
        "    #forward propagation\n",
        "    probs=model(data.cuda())\n",
        "\n",
        "    #loss evaluation\n",
        "    loss=criterion(probs,labels.cuda())\n",
        "    \n",
        "    #accumulating validation loss\n",
        "    val_loss+= loss.item()* data.shape[0]\n",
        "\n",
        "  #computing average loss per epoch\n",
        "  train_loss=train_loss/len(train_dataset)\n",
        "  val_loss=val_loss/len(val_dataset)\n",
        "\n",
        "  #save average train loss per epoch\n",
        "  train_losses.append(train_loss)\n",
        "  #save average validation loss per epoch\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(f\"Epoch {i}:\\t train loss:{train_loss:.6f}\\t val_loss:{val_loss:.6f}\")\n",
        "  \n",
        "  #keep track of the weights of the model with the lowest validation loss  \n",
        "  if val_loss<= min_val_loss:\n",
        "    print(f\"val loss decreased from {min_val_loss:.6f} to {val_loss:.6f}\\n\")\n",
        "    torch.save(model.state_dict(),\"model\"+str(i)+\".pth\")\n",
        "    min_val_loss=val_loss\n",
        "    best_epoch=i"
      ],
      "metadata": {
        "id": "cyx_wvvcSkCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses,'r')\n",
        "plt.plot(val_losses,'b')\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2qYYzJF1Spur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"model\"+str(best_epoch)+\".pth\"))"
      ],
      "metadata": {
        "id": "urcnAtISSyQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "#keep track of the correct predictions\n",
        "class_correct=[0. for i in range(2)]\n",
        "\n",
        "#keep track of the total number of predictions\n",
        "class_total=[0. for i in range(2)]\n",
        "\n",
        "test_loss=0\n",
        "\n",
        "for data,labels in testloader:\n",
        "  #forward propagation\n",
        "  probs=model(data.cuda())\n",
        "  #evaluate loss\n",
        "  loss=criterion(probs,labels.cuda())\n",
        "\n",
        "  #accumulate loss over the test dataset\n",
        "  test_loss+= loss.item()*data.shape[0]\n",
        "  \n",
        "  #calculate the predicted class\n",
        "  predicted=torch.Tensor([0 if p<0.5 else 1 for p in torch.sigmoid(probs)])\n",
        "  correct=predicted.eq(labels.data)\n",
        "  \n",
        "  #save correct predictions and total predictions\n",
        "  for i in range(len(labels)):\n",
        "    label=int(labels.data[i])\n",
        "    class_correct[label]+= correct[i].item()\n",
        "    class_total[label]+=1\n",
        "\n",
        "#calculate average test loss\n",
        "test_loss=test_loss/len(test_dataset)\n",
        "print(f'test loss:  {test_loss:.6f}\\n')\n",
        "\n",
        "#calculate class accuracy\n",
        "for i in range(2):\n",
        "  acc=100*class_correct[i]/class_total[i]\n",
        "  print(f\"Test accuracy of {i}: {acc:.2f}%\")\n",
        "\n",
        "#compute average test accuracy\n",
        "overall_test_accuracy=100*np.sum(class_correct)/np.sum(class_total)\n",
        "print(f\"\\n Overall test accuracy: {overall_test_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "HBK5kZ52S1cD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}